{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dc.tif file and the original Virginia Beach GeoTIFF file\n",
    "with rasterio.open(dc_tif_path) as dc_src, rasterio.open(vb_tif_path) as vb_src:\n",
    "    # Read the metadata of each image\n",
    "    dc_metadata = dc_src.meta\n",
    "    vb_metadata = vb_src.meta\n",
    "    \n",
    "    # Update the Virginia Beach metadata to match DC metadata, except for width, height, and count\n",
    "    vb_metadata.update({\n",
    "        key: value for key, value in dc_metadata.items() if key not in ['width', 'height', 'count']\n",
    "    })\n",
    "    \n",
    "    # Save the updated Virginia Beach GeoTIFF file with the new metadata\n",
    "    with rasterio.open(\n",
    "        updated_vb_tif_path,\n",
    "        'w',\n",
    "        **vb_metadata\n",
    "    ) as dst:\n",
    "        for i in range(1, vb_src.count + 1):\n",
    "            dst.write(vb_src.read(i), i)\n",
    "    \n",
    "    # Plot the metadata of each image side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    axes[0].imshow(dc_src.read(1), cmap='gray')\n",
    "    axes[0].set_title('First Band of DC GeoTIFF Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(vb_src.read(1), cmap='gray')\n",
    "    axes[1].set_title('First Band of Virginia Beach GeoTIFF Image')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print metadata\n",
    "    print(\"DC GeoTIFF Metadata:\")\n",
    "    for key, value in dc_metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\nVirginia Beach GeoTIFF Metadata (Updated):\")\n",
    "    for key, value in vb_metadata.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Common colormaps include: 'viridis', 'plasma', 'inferno', 'magma', 'cividis', 'gray', 'hot', 'cool', 'spring', 'summer', 'autumn', 'winter'.\n",
    "\"\"\"\n",
    "def plot_hyperspectral_images_with_keys(path, keys, band=0):\n",
    "    # Load the .mat file\n",
    "    data = scipy.io.loadmat(path)\n",
    "    \n",
    "    # Extract the 3D arrays using the provided keys\n",
    "    arrays = [data[key] for key in keys]\n",
    "    \n",
    "    # Extract the specified band (assuming the third dimension is the spectral dimension)\n",
    "    bands = [array[:, :, band] for array in arrays]\n",
    "    \n",
    "    # Plot the images side by side\n",
    "    fig, axes = plt.subplots(1, len(bands), figsize=(15, 5))\n",
    "    \n",
    "    for ax, band, key in zip(axes, bands, keys):\n",
    "        ax.imshow(band, cmap='viridis')\n",
    "        ax.imshow(band, cmap='viridis', alpha=0)\n",
    "        ax.set_title(f'{key}')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "plot_hyperspectral_images_with_keys(\n",
    "    './data/HSI_Data/Hyperspectral_Project/WDC/results/hypersigma/hypersigma_Case5.mat',\n",
    "    keys=['gt', 'input', 'output'],\n",
    "    band=60  # Change this value to plot a different band\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of WDC case data: (191, 192, 192)\n",
      "Shape of HYPSO case data: (120, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "wdc_test = './data/HSI_Data/Hyperspectral_Project/WDC/test/test.mat'\n",
    "hypso_test = './data/HSI_Data/Hyperspectral_Project/HYPSO/test/test.mat'\n",
    "wdc_test_case='./data/HSI_Data/Hyperspectral_Project/WDC/test_noise/Cases/Case2/test.mat'\n",
    "hypso_test_case='./data/HSI_Data/Hyperspectral_Project/HYPSO/test_noise/Cases/Case2/test.mat'\n",
    "# Load the .mat files\n",
    "wdc_data = scipy.io.loadmat(wdc_test)\n",
    "hypso_data = scipy.io.loadmat(hypso_test)\n",
    "wdc_case_data = scipy.io.loadmat(wdc_test_case)\n",
    "hypso_case_data = scipy.io.loadmat(hypso_test_case)\n",
    "\n",
    "# BEGIN: shape of wdc case data and hypso case data\n",
    "wdc_case_shape = wdc_case_data['gt'].shape\n",
    "hypso_case_shape = hypso_case_data['gt'].shape\n",
    "\n",
    "print(\"Shape of WDC case data:\", wdc_case_shape)\n",
    "print(\"Shape of HYPSO case data:\", hypso_case_shape)\n",
    "# END:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create wdc...\n",
    "Map size(GB): 0.23591503500938416\n",
    "loading mat: train_0.mat\n",
    "-------------------\n",
    "Preprocessing data...\n",
    "Preprocess data shape: (191, 600, 307)\n",
    "zooming to 1.000000\n",
    "zooming to 0.500000\n",
    "zooming to 0.250000\n",
    "new data 0 shape  (544, 191, 64, 64)\n",
    "new data 1 shape  (360, 191, 64, 64)\n",
    "new data 2 shape  (22, 191, 64, 64)\n",
    "concatenating data...\n",
    "new data shape after conc: (926, 191, 64, 64)\n",
    "augmenting data...\n",
    "Postprocess data shape: (926, 191, 64, 64)\n",
    "/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/data/HSI_Data/Hyperspectral_Project/WDC/wdc.db\n",
    "load mat: train_0.mat\n",
    "-------------------\n",
    "Preprocessing data...\n",
    "Preprocess data shape: (191, 600, 307)\n",
    "zooming to 1.000000\n",
    "zooming to 0.500000\n",
    "zooming to 0.250000\n",
    "new data 0 shape  (544, 191, 64, 64)\n",
    "new data 1 shape  (360, 191, 64, 64)\n",
    "new data 2 shape  (22, 191, 64, 64)\n",
    "concatenating data...\n",
    "new data shape after conc: (926, 191, 64, 64)\n",
    "augmenting data...\n",
    "Postprocess data shape: (926, 191, 64, 64)\n",
    "load mat: train_1.mat\n",
    "-------------------\n",
    "Preprocessing data...\n",
    "Preprocess data shape: (191, 480, 307)\n",
    "zooming to 1.000000\n",
    "zooming to 0.500000\n",
    "zooming to 0.250000\n",
    "new data 0 shape  (432, 191, 64, 64)\n",
    "new data 1 shape  (276, 191, 64, 64)\n",
    "new data 2 shape  (16, 191, 64, 64)\n",
    "concatenating data...\n",
    "new data shape after conc: (724, 191, 64, 64)\n",
    "augmenting data...\n",
    "Postprocess data shape: (724, 191, 64, 64)\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create HYPSO...\n",
    "Map size(GB): 0.15870451927185059\n",
    "loading mat: train_0.mat\n",
    "-------------------\n",
    "Preprocessing data...\n",
    "Preprocess data shape: (120, 600, 398)\n",
    "zooming to 1.000000\n",
    "zooming to 0.500000\n",
    "zooming to 0.250000\n",
    "new data 0 shape  (714, 120, 64, 64)\n",
    "new data 1 shape  (510, 120, 64, 64)\n",
    "new data 2 shape  (55, 120, 64, 64)\n",
    "concatenating data...\n",
    "new data shape after conc: (1279, 120, 64, 64)\n",
    "augmenting data...\n",
    "Postprocess data shape: (1279, 120, 64, 64)\n",
    "/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/data/HSI_Data/Hyperspectral_Project/HYPSO2/hypso.db\n",
    "load mat: train_0.mat\n",
    "-------------------\n",
    "Preprocessing data...\n",
    "Preprocess data shape: (120, 600, 398)\n",
    "zooming to 1.000000\n",
    "zooming to 0.500000\n",
    "zooming to 0.250000\n",
    "new data 0 shape  (714, 120, 64, 64)\n",
    "new data 1 shape  (510, 120, 64, 64)\n",
    "new data 2 shape  (55, 120, 64, 64)\n",
    "concatenating data...\n",
    "new data shape after conc: (1279, 120, 64, 64)\n",
    "augmenting data...\n",
    "Postprocess data shape: (1279, 120, 64, 64)\n",
    "load mat: train_1.mat\n",
    "-------------------\n",
    "Preprocessing data...\n",
    "Preprocess data shape: (120, 292, 398)\n",
    "zooming to 1.000000\n",
    "zooming to 0.500000\n",
    "zooming to 0.250000\n",
    "new data 0 shape  (315, 120, 64, 64)\n",
    "new data 1 shape  (187, 120, 64, 64)\n",
    "new data 2 shape  (10, 120, 64, 64)\n",
    "concatenating data...\n",
    "new data shape after conc: (512, 120, 64, 64)\n",
    "augmenting data...\n",
    "Postprocess data shape: (512, 120, 64, 64)\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namespace(\n",
    "    prefix='hypersigma_gaussian', \n",
    "    arch='hypersigma', \n",
    "    batchSize=16, \n",
    "    lr=0.0001, \n",
    "    wd=0, \n",
    "    loss='l2', \n",
    "    testdir='./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5', \n",
    "    sigma=None, \n",
    "    training_dataset_path='/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/data/HSI_Data/Hyperspectral_Project/WDC/wdc_64.db', \n",
    "    pretrain='/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/pre_train/checkpoint-400.pth', \n",
    "    init='kn', \n",
    "    no_cuda=False, \n",
    "    from_scratch=False, \n",
    "    pretrain_path='/home/lofty/CODE/HyperSIGMA-fork/spat-base.pth', \n",
    "    no_log=False, \n",
    "    threads=1, \n",
    "    seed=2018, \n",
    "    resume=True, \n",
    "    no_ropt=False, \n",
    "    chop=False, \n",
    "    resumePath='./output/original_hypersigma_1e-4_spat-base_batch4_warmup_l2_epoch_1_complex_s3_8point_HYPSO2/hypersigma_gaussian/model_latest.pth', \n",
    "    dataroot='/data/HSI_Data/ICVL64_31.db', \n",
    "    clip=1000000.0, \n",
    "    gpu_ids=[0], \n",
    "    basedir='./data/HSI_Data/Hyperspectral_Project/HYPSO2/results', \n",
    "    epoch=100, \n",
    "    update_lr=5e-05, \n",
    "    meta_lr=5e-05, \n",
    "    n_way=1, \n",
    "    k_spt=2, \n",
    "    k_qry=5, \n",
    "    task_num=16, \n",
    "    update_step=5, \n",
    "    update_step_test=10\n",
    ")\n",
    "Cuda Acess: 1\n",
    "=> creating model 'hypersigma'\n",
    "load our vit fusion_new_v5 final models\n",
    "\n",
    "checkpoint = torch.load(pretrained, map_location='cpu')\n",
    "\n",
    "MSELoss()\n",
    "==> Resuming from checkpoint ./output/original_hypersigma_1e-4_spat-base_batch4_warmup_l2_epoch_1_complex_s3_8point_HYPSO2/hypersigma_gaussian/model_latest.pth..\n",
    "\n",
    "checkpoint = torch.load(resumePath or model_best_path)\n",
    "Number of parameter: 193.55M\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_8.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_0.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_6.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_3.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_5.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_2.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_7.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_4.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_1.mat\n",
    "Adjust Learning Rate => 1.0000e-04\n",
    "1727386755.7704296\n",
    "[i] Eval dataset ...\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_8.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_0.mat\n",
    "0\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_6.mat\n",
    "\n",
    "warnings.warn(\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_3.mat| Tot: 0ms | Loss: 4.6058e-03 | PSNR: 24.1270 | AVGPSNR: 24.127 1/9 \n",
    "1\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_5.matot: 97ms | Loss: 4.0231e-03 | PSNR: 26.4288 | AVGPSNR: 25.277 2/9 \n",
    "2\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_2.matot: 181ms | Loss: 3.6586e-03 | PSNR: 26.2393 | AVGPSNR: 25.598 3/9 \n",
    "3\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_7.matot: 262ms | Loss: 3.5386e-03 | PSNR: 26.3787 | AVGPSNR: 25.793 4/9 \n",
    "4\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_4.matot: 354ms | Loss: 3.5157e-03 | PSNR: 25.4158 | AVGPSNR: 25.717 5/9 \n",
    "5\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/HYPSO2/test_noise/Patch_Cases/Case5/test_1.matTot: 456ms | Loss: 3.3134e-03 | PSNR: 27.3817 | AVGPSNR: 25.995 6/9 \n",
    "6\n",
    "torch.Size([1, 120, 64, 64])\n",
    "7[===========================================>.....................]  Step: 88ms | Tot: 545ms | Loss: 3.1914e-03 | PSNR: 26.9378 | AVGPSNR: 26.129 7/9 \n",
    "torch.Size([1, 120, 64, 64])\n",
    "8[==================================================>..............]  Step: 104ms | Tot: 649ms | Loss: 3.0727e-03 | PSNR: 27.4128 | AVGPSNR: 26.290 8/9 \n",
    "torch.Size([1, 120, 64, 64])\n",
    " [=========================================================>.......]  Step: 93ms | Tot: 743ms | Loss: 2.9807e-03 | PSNR: 27.4071 | AVGPSNR: 26.414 9/9 \n",
    "26.414331939999883 13.822076299706104 0.904122065688788 9.187853931398442 19.87596045703036\n",
    "26.414331939866347 0.002980694097156326 0.16035830229520798\n",
    "cost-time:  0.03674733638763428\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----options----\n",
    "Namespace(prefix='hypersigma_gaussian', arch='hypersigma', batchSize=16, lr=0.0001, wd=0, loss='l2', testdir='./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5', sigma=None, training_dataset_path='/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/data/HSI_Data/Hyperspectral_Project/WDC/wdc_64.db', pretrain='/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/pre_train/checkpoint-400.pth', init='kn', no_cuda=False, from_scratch=False, pretrain_path='/home/lofty/CODE/HyperSIGMA-fork/spat-base.pth', no_log=False, threads=1, seed=2018, resume=True, no_ropt=False, chop=False, resumePath='./output/original_hypersigma_1e-4_spat-base_batch4_warmup_l2_epoch_1_complex_s3_8point_HYPSO2/hypersigma_gaussian/model_latest.pth', dataroot='/data/HSI_Data/ICVL64_31.db', clip=1000000.0, gpu_ids=[0], basedir='./data/HSI_Data/Hyperspectral_Project/GRIZZLY/results', epoch=100, update_lr=5e-05, meta_lr=5e-05, n_way=1, k_spt=2, k_qry=5, task_num=16, update_step=5, update_step_test=10)\n",
    "--options end--\n",
    "Cuda Acess: 1\n",
    "=> creating model 'hypersigma'\n",
    "load our vit fusion_new_v5 final models\n",
    "_IncompatibleKeys(missing_keys=['pos_embed_add', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed_add.proj.weight', 'patch_embed_add.proj.bias', 'blocks.0.attn.sampling_offsets.weight', 'blocks.0.attn.sampling_offsets.bias', 'blocks.1.attn.sampling_offsets.weight', 'blocks.1.attn.sampling_offsets.bias', 'blocks.3.attn.sampling_offsets.weight', 'blocks.3.attn.sampling_offsets.bias', 'blocks.4.attn.sampling_offsets.weight', 'blocks.4.attn.sampling_offsets.bias', 'blocks.6.attn.sampling_offsets.weight', 'blocks.6.attn.sampling_offsets.bias', 'blocks.7.attn.sampling_offsets.weight', 'blocks.7.attn.sampling_offsets.bias', 'blocks.9.attn.sampling_offsets.weight', 'blocks.9.attn.sampling_offsets.bias', 'blocks.10.attn.sampling_offsets.weight', 'blocks.10.attn.sampling_offsets.bias', 'blocks_add.0.norm1.weight', 'blocks_add.0.norm1.bias', 'blocks_add.0.attn.qkv.weight', 'blocks_add.0.attn.qkv.bias', 'blocks_add.0.attn.sampling_offsets.weight', 'blocks_add.0.attn.sampling_offsets.bias', 'blocks_add.0.attn.proj.weight', 'blocks_add.0.attn.proj.bias', 'blocks_add.0.norm2.weight', 'blocks_add.0.norm2.bias', 'blocks_add.0.mlp.fc1.weight', 'blocks_add.0.mlp.fc1.bias', 'blocks_add.0.mlp.fc2.weight', 'blocks_add.0.mlp.fc2.bias', 'blocks_add.1.norm1.weight', 'blocks_add.1.norm1.bias', 'blocks_add.1.attn.qkv.weight', 'blocks_add.1.attn.qkv.bias', 'blocks_add.1.attn.sampling_offsets.weight', 'blocks_add.1.attn.sampling_offsets.bias', 'blocks_add.1.attn.proj.weight', 'blocks_add.1.attn.proj.bias', 'blocks_add.1.norm2.weight', 'blocks_add.1.norm2.bias', 'blocks_add.1.mlp.fc1.weight', 'blocks_add.1.mlp.fc1.bias', 'blocks_add.1.mlp.fc2.weight', 'blocks_add.1.mlp.fc2.bias', 'blocks_add.2.norm1.weight', 'blocks_add.2.norm1.bias', 'blocks_add.2.attn.qkv.weight', 'blocks_add.2.attn.qkv.bias', 'blocks_add.2.attn.proj.weight', 'blocks_add.2.attn.proj.bias', 'blocks_add.2.norm2.weight', 'blocks_add.2.norm2.bias', 'blocks_add.2.mlp.fc1.weight', 'blocks_add.2.mlp.fc1.bias', 'blocks_add.2.mlp.fc2.weight', 'blocks_add.2.mlp.fc2.bias', 'blocks_add.3.norm1.weight', 'blocks_add.3.norm1.bias', 'blocks_add.3.attn.qkv.weight', 'blocks_add.3.attn.qkv.bias', 'blocks_add.3.attn.sampling_offsets.weight', 'blocks_add.3.attn.sampling_offsets.bias', 'blocks_add.3.attn.proj.weight', 'blocks_add.3.attn.proj.bias', 'blocks_add.3.norm2.weight', 'blocks_add.3.norm2.bias', 'blocks_add.3.mlp.fc1.weight', 'blocks_add.3.mlp.fc1.bias', 'blocks_add.3.mlp.fc2.weight', 'blocks_add.3.mlp.fc2.bias', 'up1.weight', 'up1.bias', 'conv1_reconstruct.weight', 'conv1_reconstruct.bias', 'conv2_reconstruct.weight', 'conv2_reconstruct.bias', 'conv3_reconstruct.weight', 'conv3_reconstruct.bias', 'conv_head.weight', 'conv_head.bias', 'conv_tail.weight', 'conv_tail.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'fpn1.0.weight', 'fpn1.0.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])\n",
    "/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/models/hypersigma/Spectral.py:468: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  checkpoint = torch.load(pretrained, map_location='cpu')\n",
    "_IncompatibleKeys(missing_keys=['blocks.0.attn.sampling_offsets.weight', 'blocks.0.attn.sampling_offsets.bias', 'blocks.1.attn.sampling_offsets.weight', 'blocks.1.attn.sampling_offsets.bias', 'blocks.3.attn.sampling_offsets.weight', 'blocks.3.attn.sampling_offsets.bias', 'blocks.4.attn.sampling_offsets.weight', 'blocks.4.attn.sampling_offsets.bias', 'blocks.6.attn.sampling_offsets.weight', 'blocks.6.attn.sampling_offsets.bias', 'blocks.7.attn.sampling_offsets.weight', 'blocks.7.attn.sampling_offsets.bias', 'blocks.9.attn.sampling_offsets.weight', 'blocks.9.attn.sampling_offsets.bias', 'blocks.10.attn.sampling_offsets.weight', 'blocks.10.attn.sampling_offsets.bias', 'conv_q.0.weight', 'conv_q.0.bias', 'conv_k.0.weight', 'conv_k.0.bias', 'conv_v.0.weight', 'conv_v.0.bias', 'l1.weight', 'conv_head.weight', 'conv_head.bias', 'conv1_reconstruct.weight', 'conv1_reconstruct.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])\n",
    "MSELoss()\n",
    "==> Resuming from checkpoint ./output/original_hypersigma_1e-4_spat-base_batch4_warmup_l2_epoch_1_complex_s3_8point_HYPSO2/hypersigma_gaussian/model_latest.pth..\n",
    "/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/hsi_setup.py:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  checkpoint = torch.load(resumePath or model_best_path)\n",
    "Number of parameter: 193.55M\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_8.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_0.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_6.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_3.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_5.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_2.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_7.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_4.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_1.mat\n",
    "Adjust Learning Rate => 1.0000e-04\n",
    "start-time:  1731932569.313687\n",
    "[i] Eval dataset ...\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_8.mat\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_0.mat\n",
    "0\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_6.mat\n",
    "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789220573/work/aten/src/ATen/native/TensorShape.cpp:3609.)\n",
    "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
    "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
    "  return fn(*args, **kwargs)\n",
    "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
    "  warnings.warn(\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_3.mat Tot: 0ms | Loss: 3.2862e-03 | PSNR: 26.0092 | AVGPSNR: 26.009 1/9 \n",
    "1\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_5.matot: 102ms | Loss: 3.4111e-03 | PSNR: 25.6181 | AVGPSNR: 25.813 2/9 \n",
    "2\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_2.matt: 192ms | Loss: 3.3810e-03 | PSNR: 25.8060 | AVGPSNR: 25.811 3/9 \n",
    "3\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_7.matt: 280ms | Loss: 3.4650e-03 | PSNR: 25.3850 | AVGPSNR: 25.704 4/9 \n",
    "4\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_4.matt: 365ms | Loss: 3.6093e-03 | PSNR: 24.9140 | AVGPSNR: 25.546 5/9 \n",
    "5\n",
    "torch.Size([1, 120, 64, 64])\n",
    "./data/HSI_Data/Hyperspectral_Project/GRIZZLY/test_noise/Patch_Cases/Case5/test_1.matt: 451ms | Loss: 3.6997e-03 | PSNR: 24.9522 | AVGPSNR: 25.447 6/9 \n",
    "6\n",
    "torch.Size([1, 120, 64, 64])\n",
    "7[===========================================>.....................]  Step: 84ms | Tot: 535ms | Loss: 3.6630e-03 | PSNR: 25.6089 | AVGPSNR: 25.470 7/9 \n",
    "torch.Size([1, 120, 64, 64])\n",
    "8[==================================================>..............]  Step: 88ms | Tot: 624ms | Loss: 3.7236e-03 | PSNR: 24.9403 | AVGPSNR: 25.404 8/9 \n",
    "torch.Size([1, 120, 64, 64])\n",
    " [=========================================================>.......]  Step: 83ms | Tot: 707ms | Loss: 3.7179e-03 | PSNR: 25.4060 | AVGPSNR: 25.404 9/9 \n",
    "25.404408713379695 15.532356410586434 0.7376596692936377 9.90338810623917 150.55429366707278\n",
    "25.404408712072627 0.0037179391106797587 0.17284672955671945\n",
    "---------\n",
    "\n",
    "BLOCK SHAPE:  (120, 64, 64)\n",
    "---------\n",
    "\n",
    "---------\n",
    "\n",
    "BLOCK SHAPE:  (120, 64, 64)\n",
    "---------\n",
    "\n",
    "---------\n",
    "\n",
    "BLOCK SHAPE:  (120, 64, 64)\n",
    "---------\n",
    "\n",
    "/home/lofty/CODE/HyperSIGMA-fork/ImageDenoising/data/HSI_Data/Hyperspectral_Project/GRIZZLY/results/hypersigma/\n",
    "cost-time:  0.024890480041503907\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
