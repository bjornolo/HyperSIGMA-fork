{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import rasterio\n",
    "import cv2\n",
    "import scipy.io\n",
    "import h5py\n",
    "from skimage import io\n",
    "from scipy.io import loadmat, savemat\n",
    "from skimage.transform import resize\n",
    "from hypso import Hypso1\n",
    "from osgeo import gdal\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save_Spat_patches(mat_image_path):\n",
    "    # Load the .mat image\n",
    "    mat_image = loadmat(mat_image_path)\n",
    "    image_data = mat_image['data']\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    image_bands, image_height, image_width = image_data.shape\n",
    "\n",
    "    # Define the patch size\n",
    "    patch_size = 64\n",
    "\n",
    "    # Randomly select a continuous 100 bands in each patch\n",
    "\n",
    "    # Split the image into patches and save them in the same directory as the input image\n",
    "    image_dir = os.path.join(os.path.dirname(mat_image_path), 'data')\n",
    "    for i in range(0, image_height, patch_size):\n",
    "        for j in range(0, image_width, patch_size):\n",
    "            if i+patch_size <= image_height and j+patch_size <= image_width:\n",
    "                start_band = np.random.randint(0, image_bands - 100)\n",
    "                patch = image_data[start_band:start_band+100, i:i+patch_size, j:j+patch_size]\n",
    "                patch_name = f'patch_{i}_{j}.mat'\n",
    "                patch_path = os.path.join(image_dir, patch_name)\n",
    "                # print(patch_path, patch.shape)\n",
    "                savemat(patch_path, {'img': patch})\n",
    "\n",
    "split_and_save_Spat_patches('./grizzly.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/timm/optim/optim_factory.py:7: FutureWarning: Importing from timm.optim.optim_factory is deprecated, please import via timm.optim\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.optim\", FutureWarning)\n",
      "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Not using distributed mode\n",
      "[15:11:59.230828] job dir: /home/lofty/CODE/HyperSIGMA-fork/Pretrain\n",
      "[15:11:59.230892] Namespace(batch_size=32,\n",
      "epochs=10,\n",
      "accum_iter=1,\n",
      "model='spat_mae_b',\n",
      "image_size=64,\n",
      "mask_ratio=0.75,\n",
      "norm_pix_loss=True,\n",
      "use_ckpt='none',\n",
      "weight_decay=0.05,\n",
      "lr=None,\n",
      "blr=0.00015,\n",
      "min_lr=0.0,\n",
      "warmup_epochs=1,\n",
      "data_path='/home/lofty/CODE/HyperSIGMA-fork/Pretrain/data',\n",
      "data_band='100bands',\n",
      "in_channels=100,\n",
      "patch_size=8,\n",
      "output_dir='/home/lofty/CODE/HyperSIGMA-fork/Pretrain/model',\n",
      "log_dir='/home/lofty/CODE/HyperSIGMA-fork/Pretrain/logs',\n",
      "device='cuda',\n",
      "seed=0,\n",
      "resume='',\n",
      "start_epoch=0,\n",
      "num_workers=4,\n",
      "pin_mem=True,\n",
      "world_size=1,\n",
      "local_rank=0,\n",
      "dist_on_itp=False,\n",
      "dist_url='env://',\n",
      "gpu_num=1,\n",
      "tag=100,\n",
      "port=None,\n",
      "distributed=False)\n",
      "[15:11:59.230948] ######### 0\n",
      "[15:11:59.233313] /home/lofty/CODE/HyperSIGMA-fork/Pretrain\n",
      "[15:11:59.233605] len_dataset 153\n",
      "[15:11:59.233659] 10_0.75_0.00015_0.05_32\n",
      "[15:11:59.233843] <util.datasets.HyperionDataset object at 0x7f09af6d8e90>\n",
      "[15:11:59.233880] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f09af6b8e30>\n",
      "[15:12:00.434848] Model = SpatMAE(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(100, 768, kernel_size=(8, 8), stride=(8, 8))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.00909090880304575)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.0181818176060915)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.027272727340459824)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.036363635212183)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.045454543083906174)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.054545458406209946)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.06363636255264282)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.0727272778749466)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.08181818574666977)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.09090909361839294)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.10000000149011612)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.00909090880304575)\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.0181818176060915)\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.027272727340459824)\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.036363635212183)\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.045454543083906174)\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.054545458406209946)\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(p=0.06363636255264282)\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_pred): Linear(in_features=512, out_features=6400, bias=True)\n",
      ")\n",
      "[15:12:00.435158] base lr: 1.50e-04\n",
      "[15:12:00.435192] actual lr: 1.87e-05\n",
      "[15:12:00.435221] accumulate grad iterations: 1\n",
      "[15:12:00.435250] effective batch size: 32\n",
      "[15:12:00.435710] AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1.875e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "/home/lofty/CODE/HyperSIGMA-fork/Pretrain/util/misc.py:269: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n",
      "[15:12:00.435944] Start training for 10 epochs\n",
      "/home/lofty/CODE/HyperSIGMA-fork/Pretrain/engine_pretrain.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "[15:12:01.592441] Epoch: [0]  [0/4]  eta: 0:00:04  lr: 0.000000  loss: 0.3366 (0.3366)  time: 1.1546  data: 0.2821  max mem: 2357\n",
      "[2024-12-06 15:12:01,592 INFO misc.py line 159 18579] Epoch: [0]  [0/4]  eta: 0:00:04  lr: 0.000000  loss: 0.3366 (0.3366)  time: 1.1546  data: 0.2821  max mem: 2357\n",
      "[15:12:01.886890] Epoch: [0]  [3/4]  eta: 0:00:00  lr: 0.000014  loss: 0.3217 (0.3234)  time: 0.3620  data: 0.0706  max mem: 2619\n",
      "[2024-12-06 15:12:01,887 INFO misc.py line 159 18579] Epoch: [0]  [3/4]  eta: 0:00:00  lr: 0.000014  loss: 0.3217 (0.3234)  time: 0.3620  data: 0.0706  max mem: 2619\n",
      "[15:12:01.901294] Epoch: [0] Total time: 0:00:01 (0.3661 s / it)\n",
      "[15:12:01.901615] Averaged stats: lr: 0.000014  loss: 0.3217 (0.3234)\n",
      "[15:12:03.789337] Epoch: [1]  [0/4]  eta: 0:00:01  lr: 0.000019  loss: 0.2782 (0.2782)  time: 0.3024  data: 0.2050  max mem: 2619\n",
      "[2024-12-06 15:12:03,789 INFO misc.py line 159 18579] Epoch: [1]  [0/4]  eta: 0:00:01  lr: 0.000019  loss: 0.2782 (0.2782)  time: 0.3024  data: 0.2050  max mem: 2619\n",
      "[15:12:04.085035] Epoch: [1]  [3/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2509 (0.2576)  time: 0.1493  data: 0.0513  max mem: 2619\n",
      "[2024-12-06 15:12:04,085 INFO misc.py line 159 18579] Epoch: [1]  [3/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2509 (0.2576)  time: 0.1493  data: 0.0513  max mem: 2619\n",
      "[15:12:04.101458] Epoch: [1] Total time: 0:00:00 (0.1537 s / it)\n",
      "[15:12:04.101520] Averaged stats: lr: 0.000018  loss: 0.2509 (0.2576)\n",
      "[15:12:04.320415] Epoch: [2]  [0/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2301 (0.2301)  time: 0.2173  data: 0.1193  max mem: 2619\n",
      "[2024-12-06 15:12:04,320 INFO misc.py line 159 18579] Epoch: [2]  [0/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2301 (0.2301)  time: 0.2173  data: 0.1193  max mem: 2619\n",
      "[15:12:04.605351] Epoch: [2]  [3/4]  eta: 0:00:00  lr: 0.000017  loss: 0.2131 (0.2185)  time: 0.1254  data: 0.0299  max mem: 2619\n",
      "[2024-12-06 15:12:04,605 INFO misc.py line 159 18579] Epoch: [2]  [3/4]  eta: 0:00:00  lr: 0.000017  loss: 0.2131 (0.2185)  time: 0.1254  data: 0.0299  max mem: 2619\n",
      "[15:12:04.622426] Epoch: [2] Total time: 0:00:00 (0.1300 s / it)\n",
      "[15:12:04.622583] Averaged stats: lr: 0.000017  loss: 0.2131 (0.2185)\n",
      "[15:12:04.879479] Epoch: [3]  [0/4]  eta: 0:00:01  lr: 0.000017  loss: 0.2009 (0.2009)  time: 0.2553  data: 0.1322  max mem: 2619\n",
      "[2024-12-06 15:12:04,879 INFO misc.py line 159 18579] Epoch: [3]  [0/4]  eta: 0:00:01  lr: 0.000017  loss: 0.2009 (0.2009)  time: 0.2553  data: 0.1322  max mem: 2619\n",
      "[15:12:05.160726] Epoch: [3]  [3/4]  eta: 0:00:00  lr: 0.000015  loss: 0.1908 (0.1933)  time: 0.1339  data: 0.0331  max mem: 2619\n",
      "[2024-12-06 15:12:05,160 INFO misc.py line 159 18579] Epoch: [3]  [3/4]  eta: 0:00:00  lr: 0.000015  loss: 0.1908 (0.1933)  time: 0.1339  data: 0.0331  max mem: 2619\n",
      "[15:12:05.176678] Epoch: [3] Total time: 0:00:00 (0.1382 s / it)\n",
      "[15:12:05.176738] Averaged stats: lr: 0.000015  loss: 0.1908 (0.1933)\n",
      "[15:12:05.387959] Epoch: [4]  [0/4]  eta: 0:00:00  lr: 0.000014  loss: 0.1809 (0.1809)  time: 0.2096  data: 0.1179  max mem: 2619\n",
      "[2024-12-06 15:12:05,388 INFO misc.py line 159 18579] Epoch: [4]  [0/4]  eta: 0:00:00  lr: 0.000014  loss: 0.1809 (0.1809)  time: 0.2096  data: 0.1179  max mem: 2619\n",
      "[15:12:05.686551] Epoch: [4]  [3/4]  eta: 0:00:00  lr: 0.000012  loss: 0.1718 (0.1746)  time: 0.1269  data: 0.0295  max mem: 2619\n",
      "[2024-12-06 15:12:05,686 INFO misc.py line 159 18579] Epoch: [4]  [3/4]  eta: 0:00:00  lr: 0.000012  loss: 0.1718 (0.1746)  time: 0.1269  data: 0.0295  max mem: 2619\n",
      "[15:12:05.701682] Epoch: [4] Total time: 0:00:00 (0.1309 s / it)\n",
      "[15:12:05.701747] Averaged stats: lr: 0.000012  loss: 0.1718 (0.1746)\n",
      "[15:12:05.938878] Epoch: [5]  [0/4]  eta: 0:00:00  lr: 0.000011  loss: 0.1656 (0.1656)  time: 0.2355  data: 0.1440  max mem: 2619\n",
      "[2024-12-06 15:12:05,939 INFO misc.py line 159 18579] Epoch: [5]  [0/4]  eta: 0:00:00  lr: 0.000011  loss: 0.1656 (0.1656)  time: 0.2355  data: 0.1440  max mem: 2619\n",
      "[15:12:06.223294] Epoch: [5]  [3/4]  eta: 0:00:00  lr: 0.000009  loss: 0.1593 (0.1611)  time: 0.1298  data: 0.0360  max mem: 2619\n",
      "[2024-12-06 15:12:06,223 INFO misc.py line 159 18579] Epoch: [5]  [3/4]  eta: 0:00:00  lr: 0.000009  loss: 0.1593 (0.1611)  time: 0.1298  data: 0.0360  max mem: 2619\n",
      "[15:12:06.240755] Epoch: [5] Total time: 0:00:00 (0.1344 s / it)\n",
      "[15:12:06.240919] Averaged stats: lr: 0.000009  loss: 0.1593 (0.1611)\n",
      "[15:12:06.478642] Epoch: [6]  [0/4]  eta: 0:00:00  lr: 0.000008  loss: 0.1561 (0.1561)  time: 0.2361  data: 0.1453  max mem: 2619\n",
      "[2024-12-06 15:12:06,478 INFO misc.py line 159 18579] Epoch: [6]  [0/4]  eta: 0:00:00  lr: 0.000008  loss: 0.1561 (0.1561)  time: 0.2361  data: 0.1453  max mem: 2619\n",
      "[15:12:06.742514] Epoch: [6]  [3/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1510 (0.1527)  time: 0.1248  data: 0.0364  max mem: 2619\n",
      "[2024-12-06 15:12:06,742 INFO misc.py line 159 18579] Epoch: [6]  [3/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1510 (0.1527)  time: 0.1248  data: 0.0364  max mem: 2619\n",
      "[15:12:06.759187] Epoch: [6] Total time: 0:00:00 (0.1292 s / it)\n",
      "[15:12:06.759364] Averaged stats: lr: 0.000005  loss: 0.1510 (0.1527)\n",
      "[15:12:06.958530] Epoch: [7]  [0/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1493 (0.1493)  time: 0.1974  data: 0.1078  max mem: 2619\n",
      "[2024-12-06 15:12:06,958 INFO misc.py line 159 18579] Epoch: [7]  [0/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1493 (0.1493)  time: 0.1974  data: 0.1078  max mem: 2619\n",
      "[15:12:07.235699] Epoch: [7]  [3/4]  eta: 0:00:00  lr: 0.000003  loss: 0.1463 (0.1472)  time: 0.1184  data: 0.0270  max mem: 2619\n",
      "[2024-12-06 15:12:07,235 INFO misc.py line 159 18579] Epoch: [7]  [3/4]  eta: 0:00:00  lr: 0.000003  loss: 0.1463 (0.1472)  time: 0.1184  data: 0.0270  max mem: 2619\n",
      "[15:12:07.256764] Epoch: [7] Total time: 0:00:00 (0.1240 s / it)\n",
      "[15:12:07.256945] Averaged stats: lr: 0.000003  loss: 0.1463 (0.1472)\n",
      "[15:12:07.502553] Epoch: [8]  [0/4]  eta: 0:00:00  lr: 0.000002  loss: 0.1436 (0.1436)  time: 0.2436  data: 0.1482  max mem: 2619\n",
      "[2024-12-06 15:12:07,502 INFO misc.py line 159 18579] Epoch: [8]  [0/4]  eta: 0:00:00  lr: 0.000002  loss: 0.1436 (0.1436)  time: 0.2436  data: 0.1482  max mem: 2619\n",
      "[15:12:07.787394] Epoch: [8]  [3/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1436 (0.1436)  time: 0.1319  data: 0.0371  max mem: 2619\n",
      "[2024-12-06 15:12:07,787 INFO misc.py line 159 18579] Epoch: [8]  [3/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1436 (0.1436)  time: 0.1319  data: 0.0371  max mem: 2619\n",
      "[15:12:07.798840] Epoch: [8] Total time: 0:00:00 (0.1351 s / it)\n",
      "[15:12:07.799063] Averaged stats: lr: 0.000001  loss: 0.1436 (0.1436)\n",
      "[15:12:08.038248] Epoch: [9]  [0/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1423 (0.1423)  time: 0.2373  data: 0.1479  max mem: 2619\n",
      "[2024-12-06 15:12:08,038 INFO misc.py line 159 18579] Epoch: [9]  [0/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1423 (0.1423)  time: 0.2373  data: 0.1479  max mem: 2619\n",
      "[15:12:08.308986] Epoch: [9]  [3/4]  eta: 0:00:00  lr: 0.000000  loss: 0.1427 (0.1428)  time: 0.1268  data: 0.0370  max mem: 2619\n",
      "[2024-12-06 15:12:08,309 INFO misc.py line 159 18579] Epoch: [9]  [3/4]  eta: 0:00:00  lr: 0.000000  loss: 0.1427 (0.1428)  time: 0.1268  data: 0.0370  max mem: 2619\n",
      "[15:12:08.324101] Epoch: [9] Total time: 0:00:00 (0.1309 s / it)\n",
      "[15:12:08.324171] Averaged stats: lr: 0.000000  loss: 0.1427 (0.1428)\n",
      "[15:12:09.960691] Training time 0:00:09\n"
     ]
    }
   ],
   "source": [
    "!bash pre_train_Spat.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/timm/optim/optim_factory.py:7: FutureWarning: Importing from timm.optim.optim_factory is deprecated, please import via timm.optim\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.optim\", FutureWarning)\n",
      "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "usage: MAE pre-training [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                        [--accum_iter ACCUM_ITER] [--model MODEL]\n",
      "                        [--image_size IMAGE_SIZE] [--mask_ratio MASK_RATIO]\n",
      "                        [--norm_pix_loss] [--use_ckpt USE_CKPT]\n",
      "                        [--weight_decay WEIGHT_DECAY] [--lr LR] [--blr LR]\n",
      "                        [--min_lr LR] [--warmup_epochs N]\n",
      "                        [--data_path DATA_PATH] [--in_chans IN_CHANS]\n",
      "                        [--num_tokens NUM_TOKENS] [--output_dir OUTPUT_DIR]\n",
      "                        [--log_dir LOG_DIR] [--device DEVICE] [--seed SEED]\n",
      "                        [--resume RESUME] [--start_epoch N]\n",
      "                        [--num_workers NUM_WORKERS] [--pin_mem] [--no_pin_mem]\n",
      "                        [--world_size WORLD_SIZE] [--local_rank LOCAL_RANK]\n",
      "                        [--dist_on_itp] [--dist_url DIST_URL]\n",
      "                        [--gpu_num GPU_NUM] [--tag TAG] [--port PORT]\n",
      "MAE pre-training: error: unrecognized arguments: 1\n"
     ]
    }
   ],
   "source": [
    "!bash pre_train_Spec.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/timm/optim/optim_factory.py:7: FutureWarning: Importing from timm.optim.optim_factory is deprecated, please import via timm.optim\n",
    "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.optim\", FutureWarning)\n",
    "/home/lofty/miniconda3/envs/AI/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
    "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
    "Not using distributed mode\n",
    "[15:11:59.230828] job dir: /home/lofty/CODE/HyperSIGMA-fork/Pretrain\n",
    "[15:11:59.230892] Namespace(batch_size=32,\n",
    "epochs=10,\n",
    "accum_iter=1,\n",
    "model='spat_mae_b',\n",
    "image_size=64,\n",
    "mask_ratio=0.75,\n",
    "norm_pix_loss=True,\n",
    "use_ckpt='none',\n",
    "weight_decay=0.05,\n",
    "lr=None,\n",
    "blr=0.00015,\n",
    "min_lr=0.0,\n",
    "warmup_epochs=1,\n",
    "data_path='/home/lofty/CODE/HyperSIGMA-fork/Pretrain/data',\n",
    "data_band='100bands',\n",
    "in_channels=100,\n",
    "patch_size=8,\n",
    "output_dir='/home/lofty/CODE/HyperSIGMA-fork/Pretrain/model',\n",
    "log_dir='/home/lofty/CODE/HyperSIGMA-fork/Pretrain/logs',\n",
    "device='cuda',\n",
    "seed=0,\n",
    "resume='',\n",
    "start_epoch=0,\n",
    "num_workers=4,\n",
    "pin_mem=True,\n",
    "world_size=1,\n",
    "local_rank=0,\n",
    "dist_on_itp=False,\n",
    "dist_url='env://',\n",
    "gpu_num=1,\n",
    "tag=100,\n",
    "port=None,\n",
    "distributed=False)\n",
    "[15:11:59.230948] ######### 0\n",
    "[15:11:59.233313] /home/lofty/CODE/HyperSIGMA-fork/Pretrain\n",
    "[15:11:59.233605] len_dataset 153\n",
    "[15:11:59.233659] 10_0.75_0.00015_0.05_32\n",
    "[15:11:59.233843] <util.datasets.HyperionDataset object at 0x7f09af6d8e90>\n",
    "[15:11:59.233880] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f09af6b8e30>\n",
    "[15:12:00.434848] Model = SpatMAE(\n",
    "  (patch_embed): PatchEmbed(\n",
    "    (proj): Conv2d(100, 768, kernel_size=(8, 8), stride=(8, 8))\n",
    "  )\n",
    "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
    "  (blocks): ModuleList(\n",
    "    (0): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): Identity()\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (1): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.00909090880304575)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (2): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.0181818176060915)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (3): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.027272727340459824)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (4): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.036363635212183)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (5): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.045454543083906174)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (6): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.054545458406209946)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (7): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.06363636255264282)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (8): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.0727272778749466)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (9): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.08181818574666977)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (10): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.09090909361839294)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (11): Block(\n",
    "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.10000000149011612)\n",
    "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
    "  (decoder_blocks): ModuleList(\n",
    "    (0): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): Identity()\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (1): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.00909090880304575)\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (2): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.0181818176060915)\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (3): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.027272727340459824)\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (4): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.036363635212183)\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (5): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.045454543083906174)\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (6): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.054545458406209946)\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "    (7): Block(\n",
    "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (attn): Attention(\n",
    "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
    "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "      (drop_path): DropPath(p=0.06363636255264282)\n",
    "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "      (mlp): Mlp(\n",
    "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "        (act): GELU(approximate='none')\n",
    "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "        (drop): Dropout(p=0.0, inplace=False)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
    "  (decoder_pred): Linear(in_features=512, out_features=6400, bias=True)\n",
    ")\n",
    "[15:12:00.435158] base lr: 1.50e-04\n",
    "[15:12:00.435192] actual lr: 1.87e-05\n",
    "[15:12:00.435221] accumulate grad iterations: 1\n",
    "[15:12:00.435250] effective batch size: 32\n",
    "[15:12:00.435710] AdamW (\n",
    "Parameter Group 0\n",
    "    amsgrad: False\n",
    "    betas: (0.9, 0.95)\n",
    "    capturable: False\n",
    "    differentiable: False\n",
    "    eps: 1e-08\n",
    "    foreach: None\n",
    "    fused: None\n",
    "    lr: 1.875e-05\n",
    "    maximize: False\n",
    "    weight_decay: 0.05\n",
    ")\n",
    "/home/lofty/CODE/HyperSIGMA-fork/Pretrain/util/misc.py:269: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
    "  self._scaler = torch.cuda.amp.GradScaler()\n",
    "[15:12:00.435944] Start training for 10 epochs\n",
    "/home/lofty/CODE/HyperSIGMA-fork/Pretrain/engine_pretrain.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
    "  with torch.cuda.amp.autocast(enabled=False):\n",
    "[15:12:01.592441] Epoch: [0]  [0/4]  eta: 0:00:04  lr: 0.000000  loss: 0.3366 (0.3366)  time: 1.1546  data: 0.2821  max mem: 2357\n",
    "[2024-12-06 15:12:01,592 INFO misc.py line 159 18579] Epoch: [0]  [0/4]  eta: 0:00:04  lr: 0.000000  loss: 0.3366 (0.3366)  time: 1.1546  data: 0.2821  max mem: 2357\n",
    "[15:12:01.886890] Epoch: [0]  [3/4]  eta: 0:00:00  lr: 0.000014  loss: 0.3217 (0.3234)  time: 0.3620  data: 0.0706  max mem: 2619\n",
    "[2024-12-06 15:12:01,887 INFO misc.py line 159 18579] Epoch: [0]  [3/4]  eta: 0:00:00  lr: 0.000014  loss: 0.3217 (0.3234)  time: 0.3620  data: 0.0706  max mem: 2619\n",
    "[15:12:01.901294] Epoch: [0] Total time: 0:00:01 (0.3661 s / it)\n",
    "[15:12:01.901615] Averaged stats: lr: 0.000014  loss: 0.3217 (0.3234)\n",
    "[15:12:03.789337] Epoch: [1]  [0/4]  eta: 0:00:01  lr: 0.000019  loss: 0.2782 (0.2782)  time: 0.3024  data: 0.2050  max mem: 2619\n",
    "[2024-12-06 15:12:03,789 INFO misc.py line 159 18579] Epoch: [1]  [0/4]  eta: 0:00:01  lr: 0.000019  loss: 0.2782 (0.2782)  time: 0.3024  data: 0.2050  max mem: 2619\n",
    "[15:12:04.085035] Epoch: [1]  [3/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2509 (0.2576)  time: 0.1493  data: 0.0513  max mem: 2619\n",
    "[2024-12-06 15:12:04,085 INFO misc.py line 159 18579] Epoch: [1]  [3/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2509 (0.2576)  time: 0.1493  data: 0.0513  max mem: 2619\n",
    "[15:12:04.101458] Epoch: [1] Total time: 0:00:00 (0.1537 s / it)\n",
    "[15:12:04.101520] Averaged stats: lr: 0.000018  loss: 0.2509 (0.2576)\n",
    "[15:12:04.320415] Epoch: [2]  [0/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2301 (0.2301)  time: 0.2173  data: 0.1193  max mem: 2619\n",
    "[2024-12-06 15:12:04,320 INFO misc.py line 159 18579] Epoch: [2]  [0/4]  eta: 0:00:00  lr: 0.000018  loss: 0.2301 (0.2301)  time: 0.2173  data: 0.1193  max mem: 2619\n",
    "[15:12:04.605351] Epoch: [2]  [3/4]  eta: 0:00:00  lr: 0.000017  loss: 0.2131 (0.2185)  time: 0.1254  data: 0.0299  max mem: 2619\n",
    "[2024-12-06 15:12:04,605 INFO misc.py line 159 18579] Epoch: [2]  [3/4]  eta: 0:00:00  lr: 0.000017  loss: 0.2131 (0.2185)  time: 0.1254  data: 0.0299  max mem: 2619\n",
    "[15:12:04.622426] Epoch: [2] Total time: 0:00:00 (0.1300 s / it)\n",
    "[15:12:04.622583] Averaged stats: lr: 0.000017  loss: 0.2131 (0.2185)\n",
    "[15:12:04.879479] Epoch: [3]  [0/4]  eta: 0:00:01  lr: 0.000017  loss: 0.2009 (0.2009)  time: 0.2553  data: 0.1322  max mem: 2619\n",
    "[2024-12-06 15:12:04,879 INFO misc.py line 159 18579] Epoch: [3]  [0/4]  eta: 0:00:01  lr: 0.000017  loss: 0.2009 (0.2009)  time: 0.2553  data: 0.1322  max mem: 2619\n",
    "[15:12:05.160726] Epoch: [3]  [3/4]  eta: 0:00:00  lr: 0.000015  loss: 0.1908 (0.1933)  time: 0.1339  data: 0.0331  max mem: 2619\n",
    "[2024-12-06 15:12:05,160 INFO misc.py line 159 18579] Epoch: [3]  [3/4]  eta: 0:00:00  lr: 0.000015  loss: 0.1908 (0.1933)  time: 0.1339  data: 0.0331  max mem: 2619\n",
    "[15:12:05.176678] Epoch: [3] Total time: 0:00:00 (0.1382 s / it)\n",
    "[15:12:05.176738] Averaged stats: lr: 0.000015  loss: 0.1908 (0.1933)\n",
    "[15:12:05.387959] Epoch: [4]  [0/4]  eta: 0:00:00  lr: 0.000014  loss: 0.1809 (0.1809)  time: 0.2096  data: 0.1179  max mem: 2619\n",
    "[2024-12-06 15:12:05,388 INFO misc.py line 159 18579] Epoch: [4]  [0/4]  eta: 0:00:00  lr: 0.000014  loss: 0.1809 (0.1809)  time: 0.2096  data: 0.1179  max mem: 2619\n",
    "[15:12:05.686551] Epoch: [4]  [3/4]  eta: 0:00:00  lr: 0.000012  loss: 0.1718 (0.1746)  time: 0.1269  data: 0.0295  max mem: 2619\n",
    "[2024-12-06 15:12:05,686 INFO misc.py line 159 18579] Epoch: [4]  [3/4]  eta: 0:00:00  lr: 0.000012  loss: 0.1718 (0.1746)  time: 0.1269  data: 0.0295  max mem: 2619\n",
    "[15:12:05.701682] Epoch: [4] Total time: 0:00:00 (0.1309 s / it)\n",
    "[15:12:05.701747] Averaged stats: lr: 0.000012  loss: 0.1718 (0.1746)\n",
    "[15:12:05.938878] Epoch: [5]  [0/4]  eta: 0:00:00  lr: 0.000011  loss: 0.1656 (0.1656)  time: 0.2355  data: 0.1440  max mem: 2619\n",
    "[2024-12-06 15:12:05,939 INFO misc.py line 159 18579] Epoch: [5]  [0/4]  eta: 0:00:00  lr: 0.000011  loss: 0.1656 (0.1656)  time: 0.2355  data: 0.1440  max mem: 2619\n",
    "[15:12:06.223294] Epoch: [5]  [3/4]  eta: 0:00:00  lr: 0.000009  loss: 0.1593 (0.1611)  time: 0.1298  data: 0.0360  max mem: 2619\n",
    "[2024-12-06 15:12:06,223 INFO misc.py line 159 18579] Epoch: [5]  [3/4]  eta: 0:00:00  lr: 0.000009  loss: 0.1593 (0.1611)  time: 0.1298  data: 0.0360  max mem: 2619\n",
    "[15:12:06.240755] Epoch: [5] Total time: 0:00:00 (0.1344 s / it)\n",
    "[15:12:06.240919] Averaged stats: lr: 0.000009  loss: 0.1593 (0.1611)\n",
    "[15:12:06.478642] Epoch: [6]  [0/4]  eta: 0:00:00  lr: 0.000008  loss: 0.1561 (0.1561)  time: 0.2361  data: 0.1453  max mem: 2619\n",
    "[2024-12-06 15:12:06,478 INFO misc.py line 159 18579] Epoch: [6]  [0/4]  eta: 0:00:00  lr: 0.000008  loss: 0.1561 (0.1561)  time: 0.2361  data: 0.1453  max mem: 2619\n",
    "[15:12:06.742514] Epoch: [6]  [3/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1510 (0.1527)  time: 0.1248  data: 0.0364  max mem: 2619\n",
    "[2024-12-06 15:12:06,742 INFO misc.py line 159 18579] Epoch: [6]  [3/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1510 (0.1527)  time: 0.1248  data: 0.0364  max mem: 2619\n",
    "[15:12:06.759187] Epoch: [6] Total time: 0:00:00 (0.1292 s / it)\n",
    "[15:12:06.759364] Averaged stats: lr: 0.000005  loss: 0.1510 (0.1527)\n",
    "[15:12:06.958530] Epoch: [7]  [0/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1493 (0.1493)  time: 0.1974  data: 0.1078  max mem: 2619\n",
    "[2024-12-06 15:12:06,958 INFO misc.py line 159 18579] Epoch: [7]  [0/4]  eta: 0:00:00  lr: 0.000005  loss: 0.1493 (0.1493)  time: 0.1974  data: 0.1078  max mem: 2619\n",
    "[15:12:07.235699] Epoch: [7]  [3/4]  eta: 0:00:00  lr: 0.000003  loss: 0.1463 (0.1472)  time: 0.1184  data: 0.0270  max mem: 2619\n",
    "[2024-12-06 15:12:07,235 INFO misc.py line 159 18579] Epoch: [7]  [3/4]  eta: 0:00:00  lr: 0.000003  loss: 0.1463 (0.1472)  time: 0.1184  data: 0.0270  max mem: 2619\n",
    "[15:12:07.256764] Epoch: [7] Total time: 0:00:00 (0.1240 s / it)\n",
    "[15:12:07.256945] Averaged stats: lr: 0.000003  loss: 0.1463 (0.1472)\n",
    "[15:12:07.502553] Epoch: [8]  [0/4]  eta: 0:00:00  lr: 0.000002  loss: 0.1436 (0.1436)  time: 0.2436  data: 0.1482  max mem: 2619\n",
    "[2024-12-06 15:12:07,502 INFO misc.py line 159 18579] Epoch: [8]  [0/4]  eta: 0:00:00  lr: 0.000002  loss: 0.1436 (0.1436)  time: 0.2436  data: 0.1482  max mem: 2619\n",
    "[15:12:07.787394] Epoch: [8]  [3/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1436 (0.1436)  time: 0.1319  data: 0.0371  max mem: 2619\n",
    "[2024-12-06 15:12:07,787 INFO misc.py line 159 18579] Epoch: [8]  [3/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1436 (0.1436)  time: 0.1319  data: 0.0371  max mem: 2619\n",
    "[15:12:07.798840] Epoch: [8] Total time: 0:00:00 (0.1351 s / it)\n",
    "[15:12:07.799063] Averaged stats: lr: 0.000001  loss: 0.1436 (0.1436)\n",
    "[15:12:08.038248] Epoch: [9]  [0/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1423 (0.1423)  time: 0.2373  data: 0.1479  max mem: 2619\n",
    "[2024-12-06 15:12:08,038 INFO misc.py line 159 18579] Epoch: [9]  [0/4]  eta: 0:00:00  lr: 0.000001  loss: 0.1423 (0.1423)  time: 0.2373  data: 0.1479  max mem: 2619\n",
    "[15:12:08.308986] Epoch: [9]  [3/4]  eta: 0:00:00  lr: 0.000000  loss: 0.1427 (0.1428)  time: 0.1268  data: 0.0370  max mem: 2619\n",
    "[2024-12-06 15:12:08,309 INFO misc.py line 159 18579] Epoch: [9]  [3/4]  eta: 0:00:00  lr: 0.000000  loss: 0.1427 (0.1428)  time: 0.1268  data: 0.0370  max mem: 2619\n",
    "[15:12:08.324101] Epoch: [9] Total time: 0:00:00 (0.1309 s / it)\n",
    "[15:12:08.324171] Averaged stats: lr: 0.000000  loss: 0.1427 (0.1428)\n",
    "[15:12:09.960691] Training time 0:00:09\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
